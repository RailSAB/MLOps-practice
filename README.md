# MLOps Practice

This project demonstrates a full **MLOps pipeline** for a machine learning model:  
from data preprocessing â†’ model training â†’ deployment of API and frontend via Docker.  

**Focus**: The project emphasizes pipeline automation and orchestration, featuring:
- Simple frontend interface
- Basic logistic regression model 
- Binary classification task
- Dataset: [Airline Passenger Satisfaction from Kaggle](https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction/data?select=train.csv)

---

## ğŸ”¹ Project Structure

```
â”œâ”€â”€ code
â”‚ â”œâ”€â”€ datasets
â”‚ â”œâ”€â”€ deployment # Docker + services
â”‚ â”‚ â”œâ”€â”€ api # FastAPI backend
â”‚ â”‚ â””â”€â”€ app # Streamlit frontend
â”‚ â””â”€â”€ models # training scripts
â”œâ”€â”€ data
â”‚ â”œâ”€â”€ raw # raw input data
â”‚ â””â”€â”€ processed # processed datasets
â”œâ”€â”€ models # saved models and scalers (pkl)
â”œâ”€â”€ services
â”‚ â””â”€â”€ airflow
â”‚ â”œâ”€â”€ dags # Airflow DAGs
â”‚ â””â”€â”€ logs # Airflow logs
â””â”€â”€ requirements.txt # dependencies
```

---

## ğŸ”¹ Tech Stack

- **Airflow** â€” pipeline orchestration (0.0.0.0:8080)  
- **MLflow** â€” experiment tracking (127.0.0.1:5000)  
- **FastAPI** â€” REST API for serving the model (localhost:8000)  
- **Streamlit** â€” frontend for interaction (localhost:8501)  
- **Docker Compose** â€” deployment of API & frontend services  

---

## ğŸ”¹ Setup & Run

### 1. Create virtual environments
Set up a virtual environment and install dependencies:

```bash
python3 -m venv .venv
source .venv/bin/activate
export AIRFLOW_HOME=$(pwd)/services/airflow

pip install -r requirements.txt
```

ğŸ“¦ requirements.txt includes:

```
apache-airflow
pandas
numpy
scikit-learn
mlflow-skinny
```

âš ï¸ For MLflow, it's recommended to use a separate environment, since it can cause conflicts with apache-airflow:

```bash
pip install mlflow
```

### 2. Run services

#### Airflow
```bash
airflow standalone
```
Web UI available at: http://0.0.0.0:8080

Username/password will be shown in the terminal on first startup, or you can find credentials in MLOps-practice/services/airflow/simple_auth_manager_passwords.json.generated

#### MLflow
In a separate environment:

```bash
mlflow ui
```
UI available at: http://127.0.0.1:5000

#### Docker
Install Docker and verify it's running:

```bash
docker --version
```

### 3. Airflow DAGs
DAGs are located in services/airflow/dags.
There are two ways to run pipelines:

**Step by step**

- data_pipeline_dag â€” data cleaning & encoding
- model_training_dag_simple â€” model training
- deploy_model_pipeline â€” Docker deployment

**Full pipeline**

- full_pipeline_dag â€” runs the entire flow (data â†’ training â†’ deploy)
- scheduled to run every 5 minutes

---

## ğŸ”¹ Pipeline Logic

### Data pipeline

- drop NaN values
- label & one-hot encoding
- save results in data/processed/

### Model training

- scale data
- train logistic regression (GridSearchCV)
- log metrics into MLflow
- save best model & scaler into models/

### Deployment

- build Docker images for FastAPI & Streamlit
- start containers:
  - API: http://localhost:8000
  - App: http://localhost:8501

---

## ğŸ”¹ Experiment Tracking
Open MLflow UI: http://127.0.0.1:5000
Here you can monitor:

- metrics (accuracy, precision, recall, f1)
- trained models

---

## ğŸ”¹ Automatic Scheduling
full_pipeline_dag is configured with cron `*/5 * * * *` â†’ runs every 5 minutes.
The pipeline executes in sequence:

1. prepare data
2. train model
3. deploy services